<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/me.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/me.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/me.png">
  <link rel="mask-icon" href="/images/me.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Times New Roman:300,300italic,400,400italic,700,700italic|Tahoma:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":5,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: true,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="The importance of a loss functionThe importance of a loss function in machine learning cannot be overstated. Whenever we try to solve a problem, there is always some expected outcome, and the actual o">
<meta property="og:type" content="article">
<meta property="og:title" content="Gradient descend for linear regression">
<meta property="og:url" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/index.html">
<meta property="og:site_name" content="Wintersweet">
<meta property="og:description" content="The importance of a loss functionThe importance of a loss function in machine learning cannot be overstated. Whenever we try to solve a problem, there is always some expected outcome, and the actual o">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient.png">
<meta property="og:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient1.png">
<meta property="og:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient2.png">
<meta property="og:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient3.png">
<meta property="og:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient3-0040620.png">
<meta property="og:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient4.png">
<meta property="article:published_time" content="2023-03-28T20:09:58.000Z">
<meta property="article:modified_time" content="2023-04-07T02:43:50.504Z">
<meta property="article:author" content="Mei Jiaojiao">
<meta property="article:tag" content="Linear regression">
<meta property="article:tag" content="Gradient descend">
<meta property="article:tag" content="Optimization">
<meta property="article:tag" content="Mean squared error">
<meta property="article:tag" content="Loss function">
<meta property="article:tag" content="Machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/gradient.png">

<link rel="canonical" href="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Gradient descend for linear regression | Wintersweet</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wintersweet</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sometimes lose, sometimes win.</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>Commonweal 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.png">
      <meta itemprop="name" content="Mei Jiaojiao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wintersweet">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Gradient descend for linear regression
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-28 20:09:58" itemprop="dateCreated datePublished" datetime="2023-03-28T20:09:58+00:00">2023-03-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-07 02:43:50" itemprop="dateModified" datetime="2023-04-07T02:43:50+00:00">2023-04-07</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2023/03/28/Gradient-descend-for-linear-regression/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/03/28/Gradient-descend-for-linear-regression/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <html><head></head><body></body></html><html><head></head><body><h3 id="The-importance-of-a-loss-function"><a href="#The-importance-of-a-loss-function" class="headerlink" title="The importance of a loss function"></a>The importance of a loss function</h3><p>The importance of a loss function in machine learning cannot be overstated. Whenever we try to solve a problem, there is always some expected outcome, and the actual outcome will inevitably have some degree of error. The goal of optimization is to minimize this error as much as possible.</p>
<p>In linear regression, we try to find a line that best fits a set of data points. However, no matter how many data points we have, there will always be some degree of error in the fit. </p>
<p><strong>When we only have a single point in the space, there are countless lines that can pass through that point, making it impossible to determine the best fit line. When we have two points, there is only one line that can pass through both points, meaning that the best fit line is well-defined. However, when we have three or more points, and those points lie on a straight line, there will always be some degree of error in the fit, no matter how we attempt to fit the line to the points.</strong></p>
<p>This is because, in this case, any line we draw will either overestimate or underestimate the data points, leading to some degree of inaccuracy in the fit. Thus, the role of the loss function is to measure this uncertainty and provide feedback to the optimization algorithm to improve the fit of the model.</p>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>A loss function is a function used to measure the difference between an expected value and a predicted value, which is also known as the loss. In simple terms, the loss function quantifies the difference between the expected value and the actual (or predicted) value.</p>
<p>The role of the loss function is to measure the quality of the model’s output by calculating the difference between the predicted output and the actual output. The goal of training a machine learning model is to minimize the loss function, which means finding the model parameters that result in the smallest possible loss.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name: Mei Jiaojiao</span></span><br><span class="line"><span class="comment"># Profession: Artificial Intelligence</span></span><br><span class="line"><span class="comment"># Time and date: 3/28/23 22:19</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate some random data for demonstration</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">y = np.array([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]) + <span class="number">2</span>* np.random.randn(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit a linear regression model to the data</span></span><br><span class="line">model = np.polyfit(x, y, <span class="number">1</span>)</span><br><span class="line">predicted = np.polyval(model, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the data and the regression line</span></span><br><span class="line">plt.scatter(x, y, color=<span class="string">'blue'</span>, label=<span class="string">'Actual'</span>)</span><br><span class="line">plt.plot(x, predicted, color=<span class="string">'red'</span>, label=<span class="string">'Predicted'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Highlight the difference between actual and predicted values</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    plt.plot([x[i], x[i]], [y[i], predicted[i]], color=<span class="string">'gray'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add labels and legend to the plot</span></span><br><span class="line">plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br><span class="line">plt.title(<span class="string">'Linear Regression'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the plot</span></span><br><span class="line">plt.savefig(<span class="string">'gradient.png'</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">'tight'</span>, pad_inches=<span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p align="center">
  <img alt="gradient" width="600" height="400" data-src="/2023/03/28/Gradient-descend-for-linear-regression/gradient.png">
</p>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name: Mei Jiaojiao</span></span><br><span class="line"><span class="comment"># Profession: Artificial Intelligence</span></span><br><span class="line"><span class="comment"># Time and date: 3/28/23 22:25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate some random data for demonstration</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">y = np.array([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]) + <span class="number">1.5</span> * np.random.randn(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit a linear regression model to the data</span></span><br><span class="line">model = np.polyfit(x, y, <span class="number">1</span>)</span><br><span class="line">predicted = np.polyval(model, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the residuals</span></span><br><span class="line">residuals = y - predicted</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the data and the regression line</span></span><br><span class="line">plt.scatter(x, y, color=<span class="string">'blue'</span>, label=<span class="string">'Actual'</span>)</span><br><span class="line">plt.plot(x, predicted, color=<span class="string">'red'</span>, label=<span class="string">'Predicted'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the orthogonal lines</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    slope = -<span class="number">1</span> / model[<span class="number">0</span>]</span><br><span class="line">    intercept = y[i] - slope * x[i]</span><br><span class="line">    intersection_x = (intercept - model[<span class="number">1</span>]) / (model[<span class="number">0</span>] - slope)</span><br><span class="line">    intersection_y = slope * intersection_x + intercept</span><br><span class="line">    plt.plot([x[i], intersection_x], [y[i], intersection_y], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add labels and legend to the plot</span></span><br><span class="line">plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br><span class="line">plt.title(<span class="string">'Linear Regression'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the plot</span></span><br><span class="line">plt.savefig(<span class="string">'gradient1.png'</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">'tight'</span>, pad_inches=<span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p align="center">
  <img alt="gradient" width="600" height="400" data-src="/2023/03/28/Gradient-descend-for-linear-regression/gradient1.png">
</p>

<h3 id="Tug-of-war"><a href="#Tug-of-war" class="headerlink" title="Tug-of-war"></a>Tug-of-war</h3><p>I had a question before about why the loss function in linear regression is represented as a vertical line instead of the distance between the point and the line. <strong>I think of linear regression as a tug-of-war game, where the points on either side of the line are pulling the line in opposite directions.</strong></p>
<p>I think about it. The vertical line is better because (1) Using a vertical line to represent the loss in linear regression is more intuitive and accurate because the goal of linear regression is to minimize the vertical distance between the predicted values and the actual values. (2) The distance between two points in a triangle is always greater than or equal to the difference between the lengths of the two sides that form the angle between them. This is known as the triangle inequality. Therefore, the use of a vertical line to represent the loss function in linear regression can be seen as a way to magnify the difference between the predicted values and the actual values, which can make it easier to see and understand the performance of the model.</p>
<p>The use of a vertical line to represent the residual or error in linear regression is a way to magnify the difference between the predicted values and the actual values, which makes it easier to measure the error or loss of the model. In the case of mean squared error (MSE), the use of the Euclidean distance (i.e., the distance between the points in a straight line) instead of the Manhattan distance (i.e., the sum of the absolute differences between the coordinates) is also a way to magnify the errors and make them more apparent. This is because the Euclidean distance gives more weight to larger errors, which can help to identify and prioritize the points with the greatest impact on the overall error.</p>
<p>Overall, the use of these techniques to magnify the errors and make them more apparent is an important step in understanding and improving the performance of the model.</p>
<h3 id="Euclidean-Distance-vs-Manhattan-distance"><a href="#Euclidean-Distance-vs-Manhattan-distance" class="headerlink" title="Euclidean Distance vs Manhattan distance"></a>Euclidean Distance vs Manhattan distance</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate some random data for demonstration</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">y = np.array([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]) + <span class="number">2</span> * np.random.randn(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># generate two outliers</span></span><br><span class="line">x = np.append(x, [<span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">y = np.append(y, [<span class="number">20</span>, <span class="number">25</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit a linear regression model to the data</span></span><br><span class="line"><span class="comment"># model1 without outliers</span></span><br><span class="line">model1 = np.polyfit(x[:-<span class="number">2</span>], y[:-<span class="number">2</span>], <span class="number">1</span>)</span><br><span class="line">predicted1 = np.polyval(model1, x[:-<span class="number">2</span>])</span><br><span class="line"><span class="comment"># model2 with outliers</span></span><br><span class="line">model2 = np.polyfit(x, y, <span class="number">1</span>)</span><br><span class="line">predicted2 = np.polyval(model2, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the data and the regression line</span></span><br><span class="line">plt.scatter(x, y, color=<span class="string">'blue'</span>, label=<span class="string">'Actual'</span>)</span><br><span class="line">plt.plot(x[:-<span class="number">2</span>], predicted1, color=<span class="string">'red'</span>, label=<span class="string">'Predicted without outliers'</span>)</span><br><span class="line">plt.plot(x, predicted2, color=<span class="string">'green'</span>, label=<span class="string">'Predicted with outliers'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Highlight the difference between actual and two predicted values</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)-<span class="number">2</span>):</span><br><span class="line">    plt.plot([x[i], x[i]], [y[i], predicted1[i]], color=<span class="string">'red'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">    plt.plot([x[i], x[i]], [y[i], predicted2[i]], color=<span class="string">'green'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add labels and legend to the plot</span></span><br><span class="line">plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br><span class="line">plt.title(<span class="string">'Linear Regression using Euclidean Distance'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the plot</span></span><br><span class="line">plt.savefig(<span class="string">'gradient2.png'</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">'tight'</span>, pad_inches=<span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p align="center">
  <img alt="gradient" width="600" height="400" data-src="/2023/03/28/Gradient-descend-for-linear-regression/gradient2.png">
</p>

<p>I added two outliers to the original data, and it is clear that these outliers have a significant impact on the regression model, causing the line to shift noticeably. This is similar to a tug-of-war game where the two outliers are pulling the line strongly towards their side. With the use of the Euclidean distance, the impact of the outliers is amplified, as the larger errors have more weight in the calculation of the loss. As the line moves to adjust to the outliers, the distance between the line and the original five points increases, leading to a larger loss. The goal is to find a balance that minimizes the overall loss.</p>
<p>If we were to use the Manhattan distance instead, the impact of the outliers may be even greater than the impact of the original five points. This is because the Manhattan distance measures the vertical distance between the line and each point, without amplifying the error through squaring.</p>
<p>In practice, the choice of distance metric depends on the number and nature of the outliers. There is no one-size-fits-all solution, and it is important to carefully consider the specific characteristics of the data and the problem at hand when choosing a distance metric for regression.</p>
<p>In general, it is a good practice to remove or handle outliers before fitting a linear regression model. </p>
<h3 id="Mean-squared-error"><a href="#Mean-squared-error" class="headerlink" title="Mean squared error"></a>Mean squared error</h3><p>Mean squared error (MSE) is a common measure of the average squared difference between the predicted values and the actual values in regression analysis. It is calculated as the average of the squared differences between the predicted and actual values for each data point. The formula for MSE is:</p>
<script type="math/tex; mode=display">
\begin{equation}
\text { MSE }=1 / n * \sum(y_i-\hat{y} _i)^2
\end{equation}</script><p>where:</p>
<ul>
<li>n is the number of data points</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 817 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> is the actual value for the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>-th data point</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="2.296ex" role="img" focusable="false" viewBox="0 -810 817 1015"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> is the predicted value for the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>-th data point</li>
</ul>
<p>The <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.62ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1600 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> term in the MSE formula is used to normalize the sum of squared errors and ensure that the MSE is relative to the number of data points in the sample. </p>
<p>For example, if we compare the MSE values for two datasets with 100 and 1000 data points, respectively, the MSE value for the larger dataset would be 10 times larger than the MSE value for the smaller dataset, even if the models have the same level of accuracy. By dividing the sum of squared errors by the number of data points, we can obtain an average error value that is more representative of the model’s performance across different sample sizes.</p>
<p>In some cases, the MSE is defined with an additional factor of 1/2 to simplify the derivative calculation, as the derivative of the squared term will cancel out the 2 in the denominator. </p>
<script type="math/tex; mode=display">
\begin{equation}
M S E=1 / 2 n * \Sigma(y_i-\hat{y}_i)^2
\end{equation}</script><p>While the additional factor of 1/2 does not change the minimum value of the MSE, it can make the derivative calculation simpler and more efficient. </p>
<h3 id="Gradient-descend"><a href="#Gradient-descend" class="headerlink" title="Gradient descend"></a>Gradient descend</h3><p>The derivative is a scalar value that represents the rate of change of a function in one dimension, while the gradient is a vector that represents the rate of change of a function in multiple dimensions.</p>
<p>Gradient descent is an optimization algorithm used to minimize a differentiable function by iteratively adjusting its parameters in the direction of steepest descent of the function. The basic idea of gradient descent is to update the parameters in a way that minimizes the loss function.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function to minimize</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: (x - <span class="number">3.5</span>) ** <span class="number">2</span> - <span class="number">4.5</span> * x + <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the gradient function</span></span><br><span class="line">g = <span class="keyword">lambda</span> x: <span class="number">2</span> * (x - <span class="number">3.5</span>) - <span class="number">4.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate x values for plotting</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">11.5</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute y values for plotting</span></span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the function</span></span><br><span class="line">plt.plot(x, y, label=<span class="string">'f(x)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the initial guess for the minimum</span></span><br><span class="line">x_min = <span class="number">5.75</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the learning rate</span></span><br><span class="line">eta = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform gradient descent</span></span><br><span class="line">x_current = np.random.randint(<span class="number">0</span>, <span class="number">12</span>, size=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">x_history = [x_current]</span><br><span class="line">tolerance = <span class="number">0.0001</span></span><br><span class="line">iteration = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    x_previous = x_current</span><br><span class="line">    x_current = x_previous - eta * g(x_previous)</span><br><span class="line">    x_history.append(x_current)</span><br><span class="line">    iteration += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">abs</span>(x_current - x_previous) &lt; tolerance:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Plot the function</span></span><br><span class="line">plt.plot(x, y, label=<span class="string">'f(x)'</span>)</span><br><span class="line"><span class="comment"># Plot the trajectory of gradient descent</span></span><br><span class="line">plt.scatter(x_history, f(np.array(x_history)), color=<span class="string">'blue'</span>, label=<span class="string">'Trajectory'</span>)</span><br><span class="line">plt.plot(x_history, f(np.array(x_history)), color=<span class="string">'blue'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line"><span class="comment"># Add labels and legend to the plot</span></span><br><span class="line">plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br><span class="line">plt.title(<span class="string">'Gradient Descent'</span>)</span><br><span class="line"><span class="comment"># iterations, learning rate, and tolerance on the right upper corner</span></span><br><span class="line">plt.text(<span class="number">6</span>, <span class="number">20</span>, <span class="string">'Iterations: {}'</span>.<span class="built_in">format</span>(iteration), fontsize=<span class="number">12</span>)</span><br><span class="line">plt.text(<span class="number">6</span>, <span class="number">18</span>, <span class="string">'Learning rate: {}'</span>.<span class="built_in">format</span>(eta), fontsize=<span class="number">12</span>)</span><br><span class="line">plt.text(<span class="number">6</span>, <span class="number">16</span>, <span class="string">'Tolerance: {}'</span>.<span class="built_in">format</span>(tolerance), fontsize=<span class="number">12</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'gradient3.png'</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">'tight'</span>, pad_inches=<span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p align="center">
  <img alt="gradient" width="500" height="400" data-src="/2023/03/28/Gradient-descend-for-linear-regression/gradient3.png">
</p>

<p align="center">
  <img alt="gradient" width="500" height="400" data-src="/2023/03/28/Gradient-descend-for-linear-regression/gradient3-0040620.png">
</p>

<p align="center">
  <img alt="gradient" width="500" height="400" data-src="/2023/03/28/Gradient-descend-for-linear-regression/gradient4.png">
</p>

<p>Learning rate is a hyperparameter that controls the step size taken during each iteration of the gradient descent optimization algorithm. Specifically, it determines how much the model parameters are updated in the direction of the negative gradient of the loss function. A high learning rate results in larger parameter updates and faster convergence, but can also cause the algorithm to overshoot the minimum and oscillate around it, or even diverge. Conversely, a low learning rate results in smaller parameter updates and slower convergence, but can also help the algorithm converge more stably and avoid overshooting.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>Ng, A. (2017). Gradient Descent. In Machine Learning (Week 2). Stanford University. Coursera. <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></li>
<li>Wikipedia contributors. (2023, March 17). Mean squared error. In Wikipedia, The Free Encyclopedia. Retrieved 14:37, March 28, 2023, from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mean_squared_error">https://en.wikipedia.org/wiki/Mean_squared_error</a></li>
</ol>
</body></html>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Mei Jiaojiao
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/" title="Gradient descend for linear regression">http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Linear-regression/" rel="tag"># Linear regression</a>
              <a href="/tags/Gradient-descend/" rel="tag"># Gradient descend</a>
              <a href="/tags/Optimization/" rel="tag"># Optimization</a>
              <a href="/tags/Mean-squared-error/" rel="tag"># Mean squared error</a>
              <a href="/tags/Loss-function/" rel="tag"># Loss function</a>
              <a href="/tags/Machine-learning/" rel="tag"># Machine learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2023/03/28/Regression/" rel="next" title="Regression">
                  <i class="fa fa-chevron-left"></i> Regression
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2023/03/29/From-linear-regression-to-binary-classification/" rel="prev" title="From linear regression to binary classification">
                  From linear regression to binary classification <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments" id="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-importance-of-a-loss-function"><span class="nav-number">1.</span> <span class="nav-text">The importance of a loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-function"><span class="nav-number">2.</span> <span class="nav-text">Loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tug-of-war"><span class="nav-number">3.</span> <span class="nav-text">Tug-of-war</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Euclidean-Distance-vs-Manhattan-distance"><span class="nav-number">4.</span> <span class="nav-text">Euclidean Distance vs Manhattan distance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-squared-error"><span class="nav-number">5.</span> <span class="nav-text">Mean squared error</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-descend"><span class="nav-number">6.</span> <span class="nav-text">Gradient descend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Mei Jiaojiao"
    src="/images/me.png">
  <p class="site-author-name" itemprop="name">Mei Jiaojiao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mei Jiaojiao</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="Symbols count total">24k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">1:27</span>
</div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>





  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://wintersweet.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  function loadComments() {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://www.meijiaojiao.love/2023/03/28/Gradient-descend-for-linear-regression/",
            identifier: "2023/03/28/Gradient-descend-for-linear-regression/",
            title: "Gradient descend for linear regression"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://wintersweet.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  }
    (function() {
      var offsetTop = document.getElementById('comments').offsetTop - window.innerHeight;
      if (offsetTop <= 0) {
        // load directly when there's no a scrollbar
        window.addEventListener('load', loadComments, false);
      } else {
        var disqus_scroll = () => {
          // offsetTop may changes because of manually resizing browser window or lazy loading images.
          var offsetTop = document.getElementById('comments').offsetTop - window.innerHeight;
          var scrollTop = window.scrollY;

          // pre-load comments a bit? (margin or anything else)
          if (offsetTop - scrollTop < 60) {
            window.removeEventListener('scroll', disqus_scroll);
            loadComments();
          }
        };
        window.addEventListener('scroll', disqus_scroll);
      }
    })();
  
</script>

</body>
</html>
